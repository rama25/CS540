In this report I shall discuss in depth about the pressing dangers of using AI. Using Generative Adversarial Networks(GAN) we can make deepfakes. In deepfakes we can replace one person's face with someone else's and create wrong data which can lead to serious issues. These content are of type, image or video which can be more appealing to end-users in believing the data. In this we can superimpose face images of a target person onto a video of a source person to make a video of the target person doing or saying things the source person does. When in wrong hands this can lead to multiple issues as it is easy to create fake news, sexual content , hoaxes, financial fraud etc to name a few. Given a training data set this technique learns how to generate new data with the same statistics or characteristics features as the training data. The newly generated output will have features which are almost similar to the training dataset and they look at least superficially authentic to human observers. While some deepfakes have been created using the traditional visual  effects or computer-graphics approaches, the latest and better accuracy and results methods are done by using either deep learning models such as autoencoders and generative adversarial networks (GAN). GAN comes with its own advantages which are really helpful such as generating image dataset, generating cartoon characters for anime shows, image to image conversion, text to image conversion, photos of emojis etc. With the current use of social media platforms it is easy to spread false data which can be created using deepfakes and make people believe that this is the actual truth. Finding the truth in the digital domain therefore has become increasingly critical. It is even more challenging when dealing with deepfakes as they are majorly used to serve malicious purposes and almost anyone can create deepfakes these days using existing deepfake tools. In the paper cited the authors even mention about the collaboration between Microsoft and Meta and have launched the Deepfake Detection Challenge to catalyze more research and development in detecting and preventing deepfakes from being used to mislead viewers. Deepfakes are created by simultaneously training two models a generative model that captures the data distribution, and a discriminative model that estimates the probability that a sample came from the training data. The training procedure for the generative model is to maximize the probability of the discriminator making a mistake. This  framework corresponds to a minimax two-player game. The advantages of using these methods are that Markov chains are never needed, only backprop is used to obtain gradients, no inference is needed during learning, and a wide variety of functions can be incorporated into the model. To classify between authentic videos and tampered ones we require a large database of real and fake videos to train classification models. The number of fake videos is increasingly available, but it is still limited in terms of setting a benchmark for validating various detection methods. Videos from the publicly available database were used to generate low and high quality deepfake videos, which can effectively mimic facial expressions, mouth movements, and eye blinking. These videos were then used to test various deepfake detection methods. Test results show that the popular face recognition systems are unable to detect deepfakes effectively. This raises concerns about the critical need for future development of more robust methods that can detect deepfakes from genuine.

